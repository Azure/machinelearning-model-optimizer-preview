{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML Optimization and Profiling\n",
    "This notebooks shows steps for creating optimization and profiling jobs on the aml platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Azure-Cli  \n",
    "  \n",
    "- Please install azure-cli with the following command:  \n",
    "`curl -sL https://aka.ms/InstallAzureCLIDeb | bash && az version`\n",
    "- Or if you already have azure-cli installed, please upgrade it with the following command:  \n",
    "`az upgrade -y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# install azure-cli ml extension\n",
    "!az extension remove --name azure-cli-ml\n",
    "!az extension remove --name ml\n",
    "!az extension add --name ml -y\n",
    "!az extension show --name ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Env setting for the workspace that runs the profiling job\n",
    "subscription_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"636d700c-4412-48fa-84be-452ac03d34a1\")\n",
    "resource_group = os.getenv(\"RESOURCE_GROUP\", default=\"model-profiler\")\n",
    "workspace_name = os.getenv(\"WORKSPACE_NAME\", default=\"profilervalidation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# !az login\n",
    "!az account set --subscription $subscription_id\n",
    "!az configure --defaults group=$resource_group workspace=$workspace_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "timestamp = int(datetime.now().timestamp())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /mnt/d/Repo/azureml-optimization-profiling/venv/lib/python3.9/site-packages (23.0.1)\n",
      "Collecting torch==1.13.1\n",
      "  Using cached torch-1.13.1-cp39-cp39-manylinux1_x86_64.whl (887.4 MB)\n",
      "Collecting torchvision==0.14.1\n",
      "  Downloading torchvision-0.14.1-cp39-cp39-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers\n",
      "  Using cached transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting typing-extensions\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.24.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Using cached Pillow-9.4.0-cp39-cp39-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.38.4-py3-none-any.whl (36 kB)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-67.6.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2022.10.31-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Using cached huggingface_hub-0.13.1-py3-none-any.whl (199 kB)\n",
      "Collecting tqdm>=4.27\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Collecting packaging>=20.0\n",
      "  Using cached packaging-23.0-py3-none-any.whl (42 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "Installing collected packages: tokenizers, wheel, urllib3, typing-extensions, tqdm, setuptools, regex, pyyaml, pillow, packaging, nvidia-cuda-nvrtc-cu11, numpy, idna, filelock, charset-normalizer, certifi, requests, nvidia-cuda-runtime-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, huggingface-hub, transformers, torch, torchvision\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.2\n",
      "    Uninstalling tokenizers-0.13.2:\n",
      "      Successfully uninstalled tokenizers-0.13.2\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.38.4\n",
      "    Uninstalling wheel-0.38.4:\n",
      "      Successfully uninstalled wheel-0.38.4\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.15\n",
      "    Uninstalling urllib3-1.26.15:\n",
      "      Successfully uninstalled urllib3-1.26.15\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.65.0\n",
      "    Uninstalling tqdm-4.65.0:\n",
      "      Successfully uninstalled tqdm-4.65.0\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 44.0.0\n",
      "    Uninstalling setuptools-44.0.0:\n",
      "      Successfully uninstalled setuptools-44.0.0\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2022.10.31\n",
      "    Uninstalling regex-2022.10.31:\n",
      "      Successfully uninstalled regex-2022.10.31\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.4.0\n",
      "    Uninstalling Pillow-9.4.0:\n",
      "      Successfully uninstalled Pillow-9.4.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.0\n",
      "    Uninstalling packaging-23.0:\n",
      "      Successfully uninstalled packaging-23.0\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu11\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu11 11.7.99\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu11-11.7.99:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu11-11.7.99\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.2\n",
      "    Uninstalling numpy-1.24.2:\n",
      "      Successfully uninstalled numpy-1.24.2\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.9.0\n",
      "    Uninstalling filelock-3.9.0:\n",
      "      Successfully uninstalled filelock-3.9.0\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 3.1.0\n",
      "    Uninstalling charset-normalizer-3.1.0:\n",
      "      Successfully uninstalled charset-normalizer-3.1.0\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2022.12.7\n",
      "    Uninstalling certifi-2022.12.7:\n",
      "      Successfully uninstalled certifi-2022.12.7\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.2\n",
      "    Uninstalling requests-2.28.2:\n",
      "      Successfully uninstalled requests-2.28.2\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu11\n",
      "    Found existing installation: nvidia-cuda-runtime-cu11 11.7.99\n",
      "    Uninstalling nvidia-cuda-runtime-cu11-11.7.99:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu11-11.7.99\n",
      "  Attempting uninstall: nvidia-cublas-cu11\n",
      "    Found existing installation: nvidia-cublas-cu11 11.10.3.66\n",
      "    Uninstalling nvidia-cublas-cu11-11.10.3.66:\n",
      "      Successfully uninstalled nvidia-cublas-cu11-11.10.3.66\n",
      "  Attempting uninstall: nvidia-cudnn-cu11\n",
      "    Found existing installation: nvidia-cudnn-cu11 8.5.0.96\n",
      "    Uninstalling nvidia-cudnn-cu11-8.5.0.96:\n",
      "      Successfully uninstalled nvidia-cudnn-cu11-8.5.0.96\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.13.1\n",
      "    Uninstalling huggingface-hub-0.13.1:\n",
      "      Successfully uninstalled huggingface-hub-0.13.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.26.1\n",
      "    Uninstalling transformers-4.26.1:\n",
      "      Successfully uninstalled transformers-4.26.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.13.1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p distilbert_model/model\n",
    "python -m pip install --upgrade pip\n",
    "python -m pip install --force-reinstall --upgrade torch==1.13.1 torchvision==0.14.1 transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.distilbert.modeling_distilbert because of the following error (look up to see its traceback):\nmodule 'torch' has no attribute 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/d/Repo/azureml-optimization-profiling/venv/lib/python3.9/site-packages/transformers/utils/import_utils.py:1110\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mimport_module(\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m module_name, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__name__\u001b[39;49m)\n\u001b[1;32m   1111\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:855\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/mnt/d/Repo/azureml-optimization-profiling/venv/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:27\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
      "File \u001b[0;32m/mnt/d/Repo/azureml-optimization-profiling/venv/lib/python3.9/site-packages/torch/nn/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F403\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mparameter\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     Parameter \u001b[39mas\u001b[39;00m Parameter,\n\u001b[1;32m      4\u001b[0m     UninitializedParameter \u001b[39mas\u001b[39;00m UninitializedParameter,\n\u001b[1;32m      5\u001b[0m     UninitializedBuffer \u001b[39mas\u001b[39;00m UninitializedBuffer,\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m/mnt/d/Repo/azureml-optimization-profiling/venv/lib/python3.9/site-packages/torch/nn/modules/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodule\u001b[39;00m \u001b[39mimport\u001b[39;00m Module\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlinear\u001b[39;00m \u001b[39mimport\u001b[39;00m Identity, Linear, Bilinear, LazyLinear\n",
      "File \u001b[0;32m/mnt/d/Repo/azureml-optimization-profiling/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:8\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparameter\u001b[39;00m \u001b[39mimport\u001b[39;00m Parameter\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhooks\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mhooks\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/d/Repo/azureml-optimization-profiling/venv/lib/python3.9/site-packages/torch/nn/parameter.py:13\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__instancecheck__\u001b[39m(instance) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m             \u001b[39misinstance\u001b[39m(instance, torch\u001b[39m.\u001b[39mTensor) \u001b[39mand\u001b[39;00m \u001b[39mgetattr\u001b[39m(instance, \u001b[39m'\u001b[39m\u001b[39m_is_param\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m))\n\u001b[0;32m---> 13\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mParameter\u001b[39;00m(torch\u001b[39m.\u001b[39;49mTensor, metaclass\u001b[39m=\u001b[39m_ParameterMeta):\n\u001b[1;32m     14\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"A kind of Tensor that is to be considered a module parameter.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[39m    Parameters are :class:`~torch.Tensor` subclasses, that have a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39m            :ref:`locally-disable-grad-doc` for more details. Default: `True`\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'Tensor'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model_checkpoint \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdistilbert-base-cased-distilled-squad\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m model \u001b[39m=\u001b[39m AutoModelForQuestionAnswering\u001b[39m.\u001b[39;49mfrom_pretrained(model_checkpoint)\n\u001b[1;32m      7\u001b[0m dynamic_axes \u001b[39m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m0\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mseq_length\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m      9\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m0\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mseq_length\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     11\u001b[0m input_tensor \u001b[39m=\u001b[39m (\n\u001b[1;32m     12\u001b[0m     torch\u001b[39m.\u001b[39mones(\u001b[39m1\u001b[39m, \u001b[39m128\u001b[39m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint64)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     13\u001b[0m     torch\u001b[39m.\u001b[39mones(\u001b[39m1\u001b[39m, \u001b[39m128\u001b[39m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint64)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m/mnt/d/Repo/azureml-optimization-profiling/venv/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:463\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    460\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    461\u001b[0m     )\n\u001b[1;32m    462\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m--> 463\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_model_mapping)\n\u001b[1;32m    464\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    465\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    466\u001b[0m     )\n\u001b[1;32m    467\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    468\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    469\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    470\u001b[0m )\n",
      "File \u001b[0;32m/mnt/d/Repo/azureml-optimization-profiling/venv/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:359\u001b[0m, in \u001b[0;36m_get_model_class\u001b[0;34m(config, model_mapping)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_model_class\u001b[39m(config, model_mapping):\n\u001b[0;32m--> 359\u001b[0m     supported_models \u001b[39m=\u001b[39m model_mapping[\u001b[39mtype\u001b[39;49m(config)]\n\u001b[1;32m    360\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(supported_models, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m    361\u001b[0m         \u001b[39mreturn\u001b[39;00m supported_models\n",
      "File \u001b[0;32m/mnt/d/Repo/azureml-optimization-profiling/venv/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:591\u001b[0m, in \u001b[0;36m_LazyAutoMapping.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[39mif\u001b[39;00m model_type \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_mapping:\n\u001b[1;32m    590\u001b[0m     model_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model_mapping[model_type]\n\u001b[0;32m--> 591\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_attr_from_module(model_type, model_name)\n\u001b[1;32m    593\u001b[0m \u001b[39m# Maybe there was several model types associated with this config.\u001b[39;00m\n\u001b[1;32m    594\u001b[0m model_types \u001b[39m=\u001b[39m [k \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_config_mapping\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m v \u001b[39m==\u001b[39m key\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m]\n",
      "File \u001b[0;32m/mnt/d/Repo/azureml-optimization-profiling/venv/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:605\u001b[0m, in \u001b[0;36m_LazyAutoMapping._load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[39mif\u001b[39;00m module_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_modules:\n\u001b[1;32m    604\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_modules[module_name] \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtransformers.models\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 605\u001b[0m \u001b[39mreturn\u001b[39;00m getattribute_from_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_modules[module_name], attr)\n",
      "File \u001b[0;32m/mnt/d/Repo/azureml-optimization-profiling/venv/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:554\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(attr, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(getattribute_from_module(module, a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m attr)\n\u001b[0;32m--> 554\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39;49m(module, attr):\n\u001b[1;32m    555\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(module, attr)\n\u001b[1;32m    556\u001b[0m \u001b[39m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[39m# object at the top level.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/d/Repo/azureml-optimization-profiling/venv/lib/python3.9/site-packages/transformers/utils/import_utils.py:1100\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_module(name)\n\u001b[1;32m   1099\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_class_to_module\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m-> 1100\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_module(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_class_to_module[name])\n\u001b[1;32m   1101\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1102\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/d/Repo/azureml-optimization-profiling/venv/lib/python3.9/site-packages/transformers/utils/import_utils.py:1112\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39mimport_module(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m module_name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m   1111\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 1112\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1113\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to import \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mmodule_name\u001b[39m}\u001b[39;00m\u001b[39m because of the following error (look up to see its\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1114\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m traceback):\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1115\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.distilbert.modeling_distilbert because of the following error (look up to see its traceback):\nmodule 'torch' has no attribute 'Tensor'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "dynamic_axes = {\n",
    "    \"input_ids\": {0: \"batch_size\", 1: \"seq_length\"},\n",
    "    \"attention_mask\": {0: \"batch_size\", 1: \"seq_length\"},\n",
    "}\n",
    "input_tensor = (\n",
    "    torch.ones(1, 128, dtype=torch.int64).to(\"cpu\"),\n",
    "    torch.ones(1, 128, dtype=torch.int64).to(\"cpu\"),\n",
    ")\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    input_tensor, \n",
    "    \"distilbert_model/model/distilbert-base-cased-distilled-squad.onnx\", \n",
    "    input_names=[\"input_ids\", \"attention_mask\"], \n",
    "    output_names=[\"start_logits\", \"end_logits\"],\n",
    "    opset_version=17,\n",
    "    dynamic_axes=dynamic_axes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Optimize your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%cd optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env OPTIMIZER_JOB_NAME=optimization-job-$timestamp\n",
    "%set_env OPTIMIZER_COMPUTE_NAME=optimizerF8\n",
    "%set_env INFERENCE_SERVICE_COMPUTE_SIZE=Standard_F8s_v2\n",
    "%set_env OPTIMIZER_DOWNLOAD_FOLDER=../downloads/optimizer_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create an Aml Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!az ml compute create --name $OPTIMIZER_COMPUTE_NAME --size $INFERENCE_SERVICE_COMPUTE_SIZE --identity-type SystemAssigned --type amlcompute"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Olive Optimizer Job Yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "envsubst '$OPTIMIZER_COMPUTE_NAME $OPTIMIZER_JOB_NAME' < optimizer_job_template.yml > optimizer_job.yml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Olive Optimizer Job and Wait Until Job To Finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "az ml job create --name $OPTIMIZER_JOB_NAME --file optimizer_job.yml\n",
    "\n",
    "status=\"\"\n",
    "while [[ ! \"$status\" =~ ^(Completed|Failed|Canceled|NotResponding|Paused)$ ]];\n",
    "do\n",
    "    status=$(az ml job show --name $OPTIMIZER_JOB_NAME --query \"status\" -o tsv)\n",
    "    echo \"Current status: $status\"\n",
    "    sleep 3\n",
    "done\n",
    "\n",
    "if [ \"$status\" != \"Completed\" ]; then echo \"Optimizer job $OPTIMIZER_JOB_NAME failed!\" && exit 1; fi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Output Files and Copy Files to The Deployer Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "az ml job download --name $OPTIMIZER_JOB_NAME --all --download-path $OPTIMIZER_DOWNLOAD_FOLDER\n",
    "\n",
    "mkdir -p distilbert_model/optimized_model\n",
    "cp $OPTIMIZER_DOWNLOAD_FOLDER/artifacts/outputs/optimized_model.onnx ../distilbert_model/optimized_model/distilbert-base-cased-distilled-squad.onnx\n",
    "cp $OPTIMIZER_DOWNLOAD_FOLDER/artifacts/outputs/optimized_parameters.json ../deployer/\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Deploy model to an oline-endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%cd ../deployer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%set_env DEPLOYER_JOB_NAME=deployer-job-$timestamp\n",
    "%set_env DEPLOYER_COMPUTE_NAME=deploymentTest\n",
    "%set_env DEPLOYER_COMPUTE_SIZE=Standard_F4s_v2\n",
    "%set_env DEPLOYER_DOWNLOAD_FOLDER=../downloads/deployer_output\n",
    "%set_env ENDPOINT_NAME=distilbert-optimized-endpt\n",
    "%set_env DEPLOYMENT_NAME=distilbert-optimized-dep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Aml Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "az ml compute create --name $DEPLOYER_COMPUTE_NAME --size $DEPLOYER_COMPUTE_SIZE --identity-type SystemAssigned --type amlcompute\n",
    "\n",
    "# Create a role-assignment\n",
    "compute_info=`az ml compute show --name $DEPLOYER_COMPUTE_NAME --query '{\"id\": id, \"identity_object_id\": identity.principal_id}' -o json`\n",
    "workspace_resource_id=`echo $compute_info | jq -r '.id' | sed 's/\\(.*\\)\\/computes\\/.*/\\1/'`\n",
    "identity_object_id=`echo $compute_info | jq -r '.identity_object_id'`\n",
    "az role assignment create --role Contributor --assignee-object-id $identity_object_id --assignee-principal-type ServicePrincipal --scope $workspace_resource_id\n",
    "if [[ $? -ne 0 ]]; then echo \"Failed to create role assignment for compute $DEPLOYER_COMPUTE_NAME\" && exit 1; fi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Deployer Job Yaml, Online-Endpoint Yaml and Online-Deployment Yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "envsubst '$DEPLOYER_COMPUTE_NAME $DEPLOYER_JOB_NAME $ENDPOINT_NAME $DEPLOYMENT_NAME' < deployer_job_template.yml > deployer_job.yml\n",
    "envsubst '$ENDPOINT_NAME $DEPLOYMENT_NAME' < endpoint_template.yml > endpoint.yml\n",
    "envsubst '$ENDPOINT_NAME $DEPLOYMENT_NAME $INFERENCE_SERVICE_COMPUTE_SIZE' < deployment_template.yml > deployment.yml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Deployer Job and Wait For Job To Finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "az ml job create --name $DEPLOYER_JOB_NAME --file deployer_job.yml\n",
    "\n",
    "status=\"\"\n",
    "while [[ ! \"$status\" =~ ^(Completed|Failed|Canceled|NotResponding|Paused)$ ]];\n",
    "do\n",
    "    status=$(az ml job show --name $DEPLOYER_JOB_NAME --query \"status\" -o tsv)\n",
    "    echo \"Current status: $status\"\n",
    "    sleep 3\n",
    "done\n",
    "\n",
    "if [ \"$status\" != \"Completed\" ]; then echo \"Deployer job $DEPLOYER_JOB_NAME failed!\" && exit 1; fi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Output Files and Copy Files to The Profiler Folder and The Deleter Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "az ml job download --name $DEPLOYER_JOB_NAME --all --download-path $DEPLOYER_DOWNLOAD_FOLDER\n",
    "cp $DEPLOYER_DOWNLOAD_FOLDER/artifacts/outputs/deployment_settings.json ../profiler/\n",
    "cp $DEPLOYER_DOWNLOAD_FOLDER/artifacts/outputs/deployment_settings.json ../deleter/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Profile your online-endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%cd ../profiler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%set_env PROFILER_JOB_NAME=profiler-job-$timestamp\n",
    "%set_env PROFILER_COMPUTE_NAME=profilingTest\n",
    "%set_env PROFILER_COMPUTE_SIZE=Standard_F4s_v2\n",
    "%set_env PROFILER_DOWNLOAD_FOLDER=../downloads/profiler_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Aml Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "az ml compute create --name $PROFILER_COMPUTE_NAME --size $PROFILER_COMPUTE_SIZE --identity-type SystemAssigned --type amlcompute\n",
    "\n",
    "# Create a role-assignment\n",
    "compute_info=`az ml compute show --name $PROFILER_COMPUTE_NAME --query '{\"id\": id, \"identity_object_id\": identity.principal_id}' -o json`\n",
    "workspace_resource_id=`echo $compute_info | jq -r '.id' | sed 's/\\(.*\\)\\/computes\\/.*/\\1/'`\n",
    "identity_object_id=`echo $compute_info | jq -r '.identity_object_id'`\n",
    "az role assignment create --role Contributor --assignee-object-id $identity_object_id --assignee-principal-type ServicePrincipal --scope $workspace_resource_id\n",
    "if [[ $? -ne 0 ]]; then echo \"Failed to create role assignment for compute $PROFILER_COMPUTE_NAME\" && exit 1; fi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Profiler Job Yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "envsubst '$PROFILER_COMPUTE_NAME $PROFILER_JOB_NAME $DEPLOYMENT_NAME' < profiler_job_template.yml > profiler_job.yml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Profiler Job and Wait For Job To Finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "az ml job create --name $PROFILER_JOB_NAME --file profiler_job.yml\n",
    "\n",
    "status=\"\"\n",
    "while [[ ! \"$status\" =~ ^(Completed|Failed|Canceled|NotResponding|Paused)$ ]];\n",
    "do\n",
    "    status=$(az ml job show --name $PROFILER_JOB_NAME --query \"status\" -o tsv)\n",
    "    echo \"Current status: $status\"\n",
    "    sleep 3\n",
    "done\n",
    "\n",
    "if [ \"$status\" != \"Completed\" ]; then echo \"Profiler job $PROFILER_JOB_NAME failed!\" && exit 1; fi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!az ml job download --name $PROFILER_JOB_NAME --all --download-path $PROFILER_DOWNLOAD_FOLDER"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Delete the online-endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%cd ../deleter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%set_env DELETER_JOB_NAME=deleter-job-$timestamp\n",
    "# will reuse the aml compute for the online-endpoint deployer job\n",
    "%set_env DELETER_COMPUTE_NAME=deploymentTest\n",
    "%set_env DELETER_DOWNLOAD_FOLDER=../downloads/deleter_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Deleter Job Yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "envsubst '$DELETER_COMPUTE_NAME $DELETER_JOB_NAME $ENDPOINT_NAME $DEPLOYMENT_NAME' < deleter_job_template.yml > deleter_job.yml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Deleter Job and Wait For Job To Finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "az ml job create --name $DELETER_JOB_NAME --file deleter_job.yml\n",
    "\n",
    "status=\"\"\n",
    "while [[ ! \"$status\" =~ ^(Completed|Failed|Canceled|NotResponding|Paused)$ ]];\n",
    "do\n",
    "    status=$(az ml job show --name $DELETER_JOB_NAME --query \"status\" -o tsv)\n",
    "    echo \"Current status: $status\"\n",
    "    sleep 3\n",
    "done\n",
    "\n",
    "if [ \"$status\" != \"Completed\" ]; then echo \"Deleter job $DELETER_JOB_NAME failed!\" && exit 1; fi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!az ml job download --name $DELETER_JOB_NAME --all --download-path $DELETER_DOWNLOAD_FOLDER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "b79f906a963c6a91675bf995089879f1fdfc2c3332e569d164e64d5005394ab2"
   }
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "69cef6aa408e6b68b41e5496a54c600635bf72566cab4eab44776299ee22ebec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
